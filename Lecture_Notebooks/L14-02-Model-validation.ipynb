{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*This notebook has been **significantly** modified from the original notebook available online, as detailed next. This notebook contains an excerpt from the [Python Data Science Handbook](http://shop.oreilly.com/product/0636920034919.do) by Jake VanderPlas; the content is available [on GitHub](https://github.com/jakevdp/PythonDataScienceHandbook).*<br>\n",
    "*The text is released under the [CC-BY-NC-ND license](https://creativecommons.org/licenses/by-nc-nd/3.0/us/legalcode), and code is released under the [MIT license](https://opensource.org/licenses/MIT).*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### The basic process for applying a supervised machine learning model:\n",
    "\n",
    "1. Choose a class of model (e.g. polynomial regression)\n",
    "2. Choose the model hyperparameters (e.g. polynomial degree)\n",
    "3. Fit the model to the training data\n",
    "4. Validate the model<br>\n",
    "   4.1 if the model is not ok, go back to either 1. or 2.<br> \n",
    "5. **Use the validated model to predict labels for new data**\n",
    "\n",
    "In order to make an informed choice, we need a way to **validate** that our **model** and our **hyperparameters** are a good fit to the data.\n",
    "\n",
    "> **There are some pitfalls that you must avoid to do this effectively**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thinking about Model Validation\n",
    "\n",
    "In principle, **model validation** is very simple: after choosing a model and its hyperparameters, we can estimate how effective it is by applying it to some of the test data and comparing the prediction to the known value.\n",
    "\n",
    "The following sections first show a naive approach to model validation and why it\n",
    "fails, before exploring the use of holdout sets and cross-validation for more robust\n",
    "model evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model validation the wrong way\n",
    "\n",
    "Let's demonstrate the naive approach to validation using the Iris data.\n",
    "\n",
    ">**We are going to use the same data for training and testing: WRONG!!**<br>\n",
    ">*We are going to get what appears to be an accurate model, which is in fact wrong!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will start by loading the data:\n",
    "    \n",
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "\n",
    "# features\n",
    "X = iris.data\n",
    "\n",
    "# variable to be predicted\n",
    "y = iris.target\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we choose a model and its hyperparameters.<br>\n",
    "**Here we'll use a *k*-neighbors classifier** with ``n_neighbors=1``. Let's not look at the specifics of this model for the time being. Intuitively, are are **setting the label of an unknown point to be the same as the label of its closest training point:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "model = KNeighborsClassifier(n_neighbors=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Then we train the model, and use it to predict labels for data we already know:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X, y)\n",
    "y_model = model.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Finally, we compute the fraction of correctly labeled points:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y, y_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see an accuracy score of 1.0, which indicates that **100% of points were correctly labeled by our model!**\n",
    "\n",
    "But is this truly measuring the expected accuracy? Have we really come upon a model that we expect to be correct 100% of the time?\n",
    "As you may have gathered, the answer is no.\n",
    "\n",
    ">In fact, this approach contains a **fundamental flaw**:\n",
    "**it trains and evaluates the model on the same data.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model validation the right way: Holdout sets\n",
    "\n",
    "So what can be done?\n",
    "A better sense of a model's performance can be found using what's known as a *holdout set*: that is, we hold back some subset of the data from the training of the model, and then use this holdout set to check the model performance.\n",
    "This splitting can be done using the ``train_test_split`` utility in ``Scikit-Learn``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9066666666666666"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Watch out!! The sklearn.cross_validation module is no longer in use\n",
    "#from sklearn.cross_validation import train_test_split\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# split the data with 50% in each set\n",
    "X1, X2, y1, y2 = train_test_split(X, y, random_state=0,\n",
    "                                  train_size=0.5)\n",
    "\n",
    "# fit the model on one training set \n",
    "model.fit(X1, y1)\n",
    "\n",
    "# evaluate the model on the testing set\n",
    "y2_model = model.predict(X2)\n",
    "accuracy_score(y2, y2_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **We see here a more reasonable result**: the nearest-neighbor classifier is about 90% accurate on this hold-out set.\n",
    "The hold-out set is similar to unknown data, because the model has not \"seen\" it before.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy classification score\n",
    "In multilabel classification, the `accuracy_score` function computes either the number or the fraction of correctly classified samples. \n",
    "> `sklearn.metrics.accuracy_score(y_true, y_pred, normalize=True, sample_weight=None)`\n",
    "\n",
    "**parameters**\n",
    "normalize : bool, optional (default=True)\n",
    "If False, return the number of correctly classified samples. Otherwise, return the fraction of correctly classified samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limitations of holdout set \n",
    ">One **disadvantage of using a holdout set** for model validation is that **we have lost a portion of our data to the model training.**\n",
    "In the above case, half the dataset does not contribute to the training of the model!\n",
    "This is not optimal, and can cause problems – especially if the initial set of training data is small."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model validation via cross-validation\n",
    "\n",
    "One way to address this is to use *cross-validation*; that is, to do a sequence of fits where each subset of the data is used both as a training set and as a validation set.\n",
    "Visually, it might look something like this:\n",
    "\n",
    "![](figures/05.03-2-fold-CV.png)\n",
    "\n",
    "Here we do two validation trials, alternately using each half of the data as a holdout set.\n",
    "Using the split data from before, we could implement it like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.96, 0.9066666666666666)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y2_model = model.fit(X1, y1).predict(X2)\n",
    "y1_model = model.fit(X2, y2).predict(X1)\n",
    "accuracy_score(y1, y1_model), accuracy_score(y2, y2_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What comes out are two accuracy scores, which we could combine (by, say, taking the mean) to get a better measure of the global model performance.\n",
    "This particular form of cross-validation is a **two-fold cross-validation** — that is, one in which we have split the data into two sets and used each in turn as a validation set.\n",
    "\n",
    "We could expand on this idea to use even more trials, and more folds in the data—for example, here is a visual depiction of **five-fold cross-validation**:\n",
    "\n",
    "![](figures/05.03-5-fold-CV.png)\n",
    "\n",
    "Here we split the data into five groups, and use each of them in turn to evaluate the model fit on the other 4/5 of the data.\n",
    "This would be rather tedious to do by hand, and so we can use Scikit-Learn's ``cross_val_score`` convenience routine to do it succinctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.96666667, 0.96666667, 0.93333333, 0.93333333, 1.        ])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#from sklearn.cross_validation import cross_val_score\n",
    "\n",
    "# five-fold cross-validation\n",
    "from sklearn.model_selection import cross_val_score\n",
    "cross_val_score(model, X, y, cv=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Repeating the validation across different subsets of the data gives us an even better idea of the performance of the algorithm.**\n",
    "\n",
    "Scikit-Learn implements a number of useful cross-validation schemes that are useful in particular situations; these are implemented via iterators in the ``cross_validation`` module.\n",
    "For example, we might wish to go to the extreme case in which our number of folds is equal to the number of data points: that is, we train on all points but one in each trial.\n",
    ">**This type of cross-validation is known as *leave-one-out* cross validation, and can be used as follows:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#from sklearn.cross_validation import LeaveOneOut\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "\n",
    "# outdated format\n",
    "#scores = cross_val_score(model, X, y, cv=LeaveOneOut(len(X)))\n",
    "\n",
    "scores = cross_val_score(model, X, y, cv=LeaveOneOut())\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we have 150 samples, the leave one out cross-validation yields scores for 150 trials, and the score indicates either successful (1.0) or unsuccessful (0.0) prediction.\n",
    "> **Taking the mean of these gives an estimate of the error rate:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.96"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Other cross-validation schemes can be used similarly.** Take a look at Scikit-Learn's online [cross-validation documentation](http://scikit-learn.org/stable/modules/cross_validation.html)."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
